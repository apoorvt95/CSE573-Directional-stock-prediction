{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCB2SfK_iqhg"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: keras in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (2.3.1)\nRequirement already satisfied: keras-applications>=1.0.6 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\nRequirement already satisfied: six>=1.9.0 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\nRequirement already satisfied: numpy>=1.9.1 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.18.1)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\nRequirement already satisfied: scipy>=0.14 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.3.1)\nRequirement already satisfied: pyyaml in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (5.1.2)\nRequirement already satisfied: h5py in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (2.9.0)\nUsing TensorFlow backend.\n"
    }
   ],
   "source": [
    "!pip install keras\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yamp8a4fiyFx"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "70000\n61611\n"
    }
   ],
   "source": [
    "df=pd.read_csv('./AmazonDataset/Label70000.csv')\n",
    "print(len(df))\n",
    "df = df[df['text'].str.split().str.len().gt(5)]\n",
    "df = df[df['text'].str.split().str.len().lt(400)]\n",
    "df=df.reset_index(drop=None)\n",
    "print(len(df))\n",
    "# news=df['text']\n",
    "# label=df['label']\n",
    "# len(news)\n",
    "# current_words=[]\n",
    "# for i in range(len(news)):\n",
    "#   # for word in news.loc[i].split(\" \"):\n",
    "#     current_words.append(news.loc[i])\n",
    "# print((current_words[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjmgbGY9JR_R"
   },
   "source": [
    "For Bigram Model we will create train and test features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ph1MUHGvdUU_"
   },
   "outputs": [],
   "source": [
    "df=df[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDyNjrwUef26"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                text  label\n0  nape summit also featur annual chariti luncheo...      1\n1  appl huge anticip iphon x samsung comeback tou...      1\n2  iphon x best phone money isnt issu appl one ex...      1\n3  appl aapl premium devic start cool per month d...      1\n4  xs spoton face id facialrecognit technolog fan...      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nape summit also featur annual chariti luncheo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>appl huge anticip iphon x samsung comeback tou...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>iphon x best phone money isnt issu appl one ex...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>appl aapl premium devic start cool per month d...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>xs spoton face id facialrecognit technolog fan...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezQLliAn8M-c"
   },
   "outputs": [],
   "source": [
    "df['word_count']=df['text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "PRX2qdel8fPK",
    "outputId": "c1bc9be6-fd2d-4865-d5f0-b18dae6445d4"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count    61611.000000\nmean        14.112399\nstd          8.429047\nmin          6.000000\n25%          9.000000\n50%         12.000000\n75%         17.000000\nmax        333.000000\nName: word_count, dtype: float64"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df.head()\n",
    "df.word_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54Z289h48vy1"
   },
   "source": [
    "Word count ranges from 11 to 2668 in the given news corpus which is huge range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "SU5P5IME89fH",
    "outputId": "d0a5d222-2716-45c1-deac-05d1ab9c5c9f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "appl      40913\ninc       15109\naapl      14816\nshare     12375\nnasdaq    10188\nrate       7740\nreport     7593\nbuy        6617\nprice      6365\niphon      6316\ndtype: int64\nNot Common words\nlinenasdaq         1\nbestlook           1\njuni               1\nhet                1\ntodayloop          1\n                  ..\npowerhousarticl    1\nactionqualcomm     1\naaplvoo            1\nbeispielsweis      1\ncashier            1\nLength: 1000, dtype: int64\n"
    }
   ],
   "source": [
    "common_words=pd.Series(''.join(df['text']).split()).value_counts()\n",
    "print(common_words[:10])\n",
    "print(\"Not Common words\")\n",
    "print(common_words[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "------> 0\n"
    }
   ],
   "source": [
    "#Removing the most frequent and least frequrbst words\n",
    "from nltk.tokenize import word_tokenize\n",
    "most_common=common_words[:7]\n",
    "least_common=common_words[-30000:]\n",
    "texts=[]\n",
    "labels=[]\n",
    "def removeWords(words):\n",
    "    final_words=[]\n",
    "    for word in words:\n",
    "        if not (word in most_common or word in least_common):\n",
    "            final_words.append(word)\n",
    "    return final_words\n",
    "for i in range(len(df)):\n",
    "    if i%100000==0:\n",
    "        print(\"------>\",i)\n",
    "    words=word_tokenize(df.loc[i].text)\n",
    "    # print(df.loc[i].text)\n",
    "    words=removeWords(words)\n",
    "    extracted_sentence=' '.join(word for word in words)\n",
    "    # print(df.loc[i].text)\n",
    "    texts.append(extracted_sentence)\n",
    "    labels.append(df.loc[i]['label'])\n",
    "new_df=pd.DataFrame({\n",
    "    'text':texts,\n",
    "    'label':labels\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "buy         6295\nprice       6257\niphon       6194\ncompani     6141\nstock       5404\nhold        5163\nresearch    4683\nmarket      4413\ntarget      3846\nnew         3524\ndtype: int64\nNot Common words\nsaidearn           3\nlinda              3\npurposfive         3\nrehlefil           3\nhubsaid            3\n                  ..\ntomacdevic         1\nsaycould           1\nstatejanuari       1\ncitizencampaign    1\ntrumpreview        1\nLength: 30000, dtype: int64\n"
    }
   ],
   "source": [
    "common_words=pd.Series(''.join(new_df['text']).split()).value_counts()\n",
    "print(common_words[:10])\n",
    "print(\"Not Common words\")\n",
    "print(common_words[-30000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count    61611.000000\nmean        11.822288\nstd          8.192491\nmin          1.000000\n25%          7.000000\n50%         10.000000\n75%         14.000000\nmax        261.000000\nName: word_count, dtype: float64"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "new_df['word_count']=new_df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "new_df.word_count.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "nape summit also featur annual chariti luncheon tom brokaw keynot speaker new job fair upstream profession educ seminar present industri organ aipn ipaa seg domest intern theater run two day prospect preview capit servic provid twoday expo nearli acr exhibit space\n"
    }
   ],
   "source": [
    "news=new_df['text']\n",
    "label=new_df['label']\n",
    "len(news)\n",
    "current_words=[]\n",
    "for i in range(len(news)):\n",
    "  # for word in news.loc[i].split(\" \"):\n",
    "    current_words.append(news.loc[i])\n",
    "print((current_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "nUOdgSYSJaL8",
    "outputId": "6901ac29-d898-4d6e-9ef0-1601ac002ad9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package omw to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /Users/akshaykumar/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "cnt_vectorizer = CountVectorizer(max_df=0.8,min_df=0.02,stop_words=stop_words,\n",
    "            ngram_range=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzIWwaLcOkvH"
   },
   "outputs": [],
   "source": [
    "X_Dataset=cnt_vectorizer.fit_transform(current_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "jGs-dewc_-Qz",
    "outputId": "f460c5e9-758a-4fea-c40a-22326e0dc2c0"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['nd quarter',\n 'second quarter',\n 'set price',\n 'price object',\n 'research note',\n 'target price',\n 'price target']"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "list(cnt_vectorizer.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3bGwRd0cFfPq"
   },
   "source": [
    "Bi gram model for plot Bar graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAHbBNQZ_aPI"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "X=tfidf.fit_transform(X_Dataset).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "j7oA_Z5tDulx",
    "outputId": "20a2edc6-7046-4e48-9461-a378da2b6ac4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "huge anticip iphon x samsung comeback tour follow note debacl big year smartphon industri\n"
    }
   ],
   "source": [
    "d=current_words[1]\n",
    "print(d)\n",
    "tfidf_vector=tfidf.transform(cnt_vectorizer.transform([d]))\n",
    "# print(tfidf_vector.toarray()[0][17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(61611, 7)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ot09juVFO6a9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.21646509 0.20496981 0.17492736]\n"
    }
   ],
   "source": [
    "trainX=df['text']\n",
    "trainY=df['label']\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pComponents= pca.fit_transform(X)\n",
    "# print((pca.components_ ))\n",
    "print(pca.explained_variance_ratio_)\n",
    "X_train, X_test, y_train, y_test = train_test_split(pComponents, trainY, test_size=0.2, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UhS28yUiPT1x",
    "outputId": "79130bdd-a7b1-4c5a-cc91-9943c1fe87f2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(49288, 3)\n\n"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CKJq4w3lP8Ao",
    "outputId": "f29a53ad-f391-4a39-8e2a-e67a99536e3a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(49288, 3)\n0.6704536233060131\n"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "clf = LogisticRegression(random_state=10).fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XwMJBv7gMLOH",
    "outputId": "bed381c5-95d8-4ffe-aafb-f50460030c53"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(49288, 3)\n0.6602288403797777\n"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state=0).fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6602288403797777\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=1000).fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Ivgd_7BxPiwX",
    "outputId": "fd57e989-c764-40ab-b144-e6b843548745"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# rf = RandomForestClassifier(random_state=43, n_jobs=-1)\n",
    "# param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10], \"min_samples_split\" : [2, 4, 10, 12, 16], \"n_estimators\": [50, 100, 400, 700, 1000]}\n",
    "# gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "# gs = gs.fit(X_train, y_train)\n",
    "# accuracy=gs.best_score_\n",
    "# print(accuracy)\n",
    "# print(gs.best_params_)\n",
    "# print(g_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RQqJbmjWkckl",
    "outputId": "75cdcbf0-28a5-4e7a-bdd8-6458f2f8b081"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "len(label)==len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRjod7688Q4l"
   },
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_95McVojEZy"
   },
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer()\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = len(max(x, key=len))\n",
    "\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYKRdBUOjKum"
   },
   "outputs": [],
   "source": [
    "tokenize_text=[]\n",
    "news_label=[]\n",
    "for n_z in news:\n",
    "  tokenize_text.append(n_z)\n",
    "for l in label:\n",
    "  news_label.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "1QLgjPMXjSuO",
    "outputId": "0ac054f5-4dd5-4685-c387-86b2b5c460af"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "nape summit also featur annual chariti luncheon tom brokaw keynot speaker new job fair upstream profession educ seminar present industri organ aipn ipaa seg aapl domest intern theater run two day prospect preview capit servic provid twoday expo nearli acr exhibit space\n1\n"
    }
   ],
   "source": [
    "print(tokenize_text[0])\n",
    "print(news_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uDT_uW9qyoej",
    "outputId": "9b150f71-641b-4b31-a2a1-9a365d7b7187"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "8238\n8238\n"
    }
   ],
   "source": [
    "news_counter = Counter(set(current_words))\n",
    "vocab_size=((len(news_counter)))\n",
    "print(vocab_size)\n",
    "print((len(set(current_words))))\n",
    "# print(news_counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVf8Y6ZEjZ7v"
   },
   "outputs": [],
   "source": [
    "text_tokenized, text_tokenizer=tokenize(tokenize_text)\n",
    "text_padded=pad(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yEkmbROdlO7O",
    "outputId": "c70e8a87-0d46-4d89-a337-cce0b14f078d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(12001, 1278)\n"
    }
   ],
   "source": [
    "print(text_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mo2I0fDQjmcu"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(12001, 1278)\n[3530 3061   40 ...    0    0    0]\n(1278,)\n[0 1]\n"
    }
   ],
   "source": [
    "print((text_padded.shape))\n",
    "print((text_padded[0]))\n",
    "trainX=text_padded\n",
    "# trainX = np.reshape(text_padded, (text_padded.shape[0], 1, 2055))\n",
    "print(trainX[0].shape)\n",
    "trainY = pd.get_dummies(news_label).values\n",
    "print(trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wh4ZrMbvlVwh"
   },
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(current_words)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_docs = t.texts_to_sequences(current_words)\n",
    "max_length = 1278\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "borOYKhvwaS6",
    "outputId": "d21d7ae9-8e80-428f-a0f6-199cd73f2fde"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(12001, 1278)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "padded_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XXtveI3RteMF",
    "outputId": "39bfa418-cebf-4d98-bb01-ff09a5080420"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Glove.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b6cab4c58648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_glove_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Glove.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b6cab4c58648>\u001b[0m in \u001b[0;36mload_glove_vector\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_glove_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Glove.txt'"
     ]
    }
   ],
   "source": [
    "from numpy import zeros\n",
    "def load_glove_vector(file):\n",
    "    f = open(file,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"words loaded\")\n",
    "    return model\n",
    "\n",
    "model=load_glove_vector('/content/drive/My Drive/Glove.txt')\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "  # print(word,i)\n",
    "  embedding_vector = model.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LBQnl3_ClCSM",
    "outputId": "c9aa76b9-cb81-422c-cf9b-e37f73b20196"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(12001, 2)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainY = pd.get_dummies(label.values).values\n",
    "print(trainY.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, trainY, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NZC_UW2I5Ixi",
    "outputId": "5bdfb823-c63c-485a-9d8b-e85ff4a2c976"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(9600, 1278)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FhsYR6UD4diZ",
    "outputId": "d53386f9-8730-467b-8266-a071de2900ab"
   },
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "trainX = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "P8bTP0c0laDn",
    "outputId": "5a488323-b078-4545-87ea-ccbfcc49bca3"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Please provide as model targets either a single array or a list of arrays. You passed: y=31491    1\n33051    1\n4236     1\n16774    0\n21432    1\n        ..\n11532    1\n27640    1\n14501    1\n30727    0\n11590    0\nName: label, Length: 34926, dtype: int64",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-2c84d9be128a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    527\u001b[0m                                          \u001b[0;34m'either a single '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                                          \u001b[0;34m'array or a list of arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m                                          'You passed: y=' + str(y))\n\u001b[0m\u001b[1;32m    530\u001b[0m                 \u001b[0;31m# Typecheck that all inputs are *either* value *or* symbolic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Please provide as model targets either a single array or a list of arrays. You passed: y=31491    1\n33051    1\n4236     1\n16774    0\n21432    1\n        ..\n11532    1\n27640    1\n14501    1\n30727    0\n11590    0\nName: label, Length: 34926, dtype: int64"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow==1.14.0\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Embedding\n",
    "model = Sequential()\n",
    "# e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=1500, trainable=False)\n",
    "# model.add(e)\n",
    "model.add(Bidirectional(LSTM(128,input_shape=(1,X_train.shape[1]),dropout=0.2)))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01),metrics=['accuracy'])\n",
    "model.fit(trainX, y_train, epochs=30, batch_size=32, verbose=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "r1tuvli1wnq-",
    "outputId": "a2544590-dc0d-49fb-acbd-ba197ffa2a79"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9600/9600 [==============================] - 0s 45us/step\nTest score: 0.6195119785269102\nTest accuracy: 0.6959375143051147\n"
    }
   ],
   "source": [
    "score, acc = model.evaluate(trainX, y_train)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "RukRG3nBlfdp",
    "outputId": "3926cce4-ff34-4029-a287-5da5620bc9d4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2401/2401 [==============================] - 0s 58us/step\nTest score: 0.6255119546087321\nTest accuracy: 0.6905456185340881\n"
    }
   ],
   "source": [
    "testX=np.reshape(X_test, (X_test.shape[0], 1, X_train.shape[1]))\n",
    "score, acc = model.evaluate(testX, y_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnojgPc3ln54"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "TrainModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}