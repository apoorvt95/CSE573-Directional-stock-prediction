{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: keras in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (2.3.1)\nRequirement already satisfied: six>=1.9.0 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\nRequirement already satisfied: h5py in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (2.9.0)\nRequirement already satisfied: scipy>=0.14 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.3.1)\nRequirement already satisfied: keras-applications>=1.0.6 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\nRequirement already satisfied: numpy>=1.9.1 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.18.1)\nRequirement already satisfied: pyyaml in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (5.1.2)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /Users/akshaykumar/anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\nUsing TensorFlow backend.\n"
    }
   ],
   "source": [
    "!pip install keras\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10000\n10000\n"
    }
   ],
   "source": [
    "df=pd.read_csv('Label10000.csv')\n",
    "print(len(df))\n",
    "# df = df[df['text'].str.split().str.len().gt(5)]\n",
    "# df = df[df['text'].str.split().str.len().lt(200)]\n",
    "df=df.reset_index(drop=None)\n",
    "print(len(df))\n",
    "# news=df['text']\n",
    "# label=df['label']\n",
    "# len(news)\n",
    "# current_words=[]\n",
    "# for i in range(len(news)):\n",
    "#   # for word in news.loc[i].split(\" \"):\n",
    "#     current_words.append(news.loc[i])\n",
    "# print((current_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                text  label\n0  nape summit also featur annual chariti luncheo...      1\n1  appl huge anticip iphon x samsung comeback tou...      0\n2  iphon x best phone money isnt issu appl one ex...      0\n3  appl aapl premium devic start cool per month d...      0\n4  xs spoton face id facialrecognit technolog fan...      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nape summit also featur annual chariti luncheo...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>appl huge anticip iphon x samsung comeback tou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>iphon x best phone money isnt issu appl one ex...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>appl aapl premium devic start cool per month d...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>xs spoton face id facialrecognit technolog fan...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df=df[['text','label']]\n",
    "df.loc[1]\n",
    "# df=df[20000::]\n",
    "# df=df.reset_index(drop=None)\n",
    "# df.head()\n",
    "# df=df[['text','label']]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "appl      5971\naapl      2096\ninc       2077\nshare     1790\nnasdaq    1219\nbuy        978\niphon      973\nrate       963\nreport     949\nprice      917\ndtype: int64\nNot Common words\nibb           3\nnarrow        3\nandv          3\nfold          3\nnktr          3\n             ..\nyearmiss      1\neanotabl      1\nnaley         1\nbanksay       1\nrumrosappl    1\nLength: 10000, dtype: int64\n"
    }
   ],
   "source": [
    "df['word_count']=df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df.word_count.describe()\n",
    "common_words=pd.Series(''.join(df['text']).split()).value_counts()\n",
    "print(common_words[:10])\n",
    "print(\"Not Common words\")\n",
    "print(common_words[-10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "------> 0\n"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "most_common=common_words[:3]\n",
    "least_common=common_words[-1000:]\n",
    "texts=[]\n",
    "labels=[]\n",
    "def removeWords(words):\n",
    "    final_words=[]\n",
    "    for word in words:\n",
    "        if not (word in most_common or word in least_common):\n",
    "            final_words.append(word)\n",
    "    return final_words\n",
    "for i in range(len(df)):\n",
    "    if i%10000==0:\n",
    "        print(\"------>\",i)\n",
    "    words=word_tokenize(df.loc[i].text)\n",
    "    # print(df.loc[i].text)\n",
    "    words=removeWords(words)\n",
    "    extracted_sentence=' '.join(word for word in words)\n",
    "    # print(df.loc[i].text)\n",
    "    texts.append(extracted_sentence)\n",
    "    labels.append(df.loc[i]['label'])\n",
    "new_df=pd.DataFrame({\n",
    "    'text':texts,\n",
    "    'label':labels\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "text     xs spoton face id facialrecognit technolog fan...\nlabel                                                    0\nName: 4, dtype: object"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "new_df.loc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer()\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = len(max(x, key=len))\n",
    "\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "huge anticip iphon x samsung comeback follow note debacl big year smartphon industri\n1\n"
    }
   ],
   "source": [
    "tokenize_text=[]\n",
    "news_label=[]\n",
    "for n_z in new_df['text']:\n",
    "  tokenize_text.append(n_z)\n",
    "for l in new_df['label']:\n",
    "  news_label.append(l)\n",
    "\n",
    "print(tokenize_text[1])\n",
    "print(news_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenized, text_tokenizer=tokenize(tokenize_text)\n",
    "text_padded=pad(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2512, 2009, 37, 366, 592, 1706, 2513, 473, 2514, 2515, 135, 19, 514, 1200, 2516, 200, 938, 2517, 579, 229, 824, 2518, 2519, 2520, 1841, 760, 2219, 474, 110, 117, 1707, 2010, 40, 22, 413, 2220, 2521, 352, 2522, 1842, 761]\n"
    }
   ],
   "source": [
    "print(text_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(10000, 1275)\n[2512 2009   37 ...    0    0    0]\n(10000, 1275)\n[0 1]\n"
    }
   ],
   "source": [
    "print((text_padded.shape))\n",
    "print((text_padded[0]))\n",
    "trainX=text_padded\n",
    "print(trainX.shape)\n",
    "trainY = pd.get_dummies(news_label).values\n",
    "print(trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import zeros\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# t = Tokenizer()\n",
    "# t.fit_on_texts(current_words)\n",
    "# vocab_size = len(t.word_index) + 1\n",
    "# encoded_docs = t.texts_to_sequences(current_words)\n",
    "# max_length = trainX[0].shape[0]\n",
    "# padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# trainY = pd.get_dummies(label.values).values\n",
    "# print(trainY.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainX, trainY, test_size=0.2, random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(8000, 1, 1275)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "trainX = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "testX = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 6400 samples, validate on 1600 samples\nEpoch 1/40\n - 1s - loss: 0.6901 - accuracy: 0.5417 - val_loss: 0.6657 - val_accuracy: 0.6050\nEpoch 2/40\n - 1s - loss: 0.6508 - accuracy: 0.6556 - val_loss: 0.6454 - val_accuracy: 0.6612\nEpoch 3/40\n - 1s - loss: 0.6327 - accuracy: 0.6831 - val_loss: 0.6380 - val_accuracy: 0.6681\nEpoch 4/40\n - 1s - loss: 0.6241 - accuracy: 0.6928 - val_loss: 0.6343 - val_accuracy: 0.6794\nEpoch 5/40\n - 1s - loss: 0.6196 - accuracy: 0.6925 - val_loss: 0.6324 - val_accuracy: 0.6800\nEpoch 6/40\n - 1s - loss: 0.6203 - accuracy: 0.6947 - val_loss: 0.6324 - val_accuracy: 0.6806\nEpoch 7/40\n - 1s - loss: 0.6133 - accuracy: 0.7016 - val_loss: 0.6316 - val_accuracy: 0.6831\nEpoch 8/40\n - 1s - loss: 0.6142 - accuracy: 0.7008 - val_loss: 0.6323 - val_accuracy: 0.6844\nEpoch 9/40\n - 1s - loss: 0.6139 - accuracy: 0.7006 - val_loss: 0.6333 - val_accuracy: 0.6844\nEpoch 10/40\n - 1s - loss: 0.6115 - accuracy: 0.7022 - val_loss: 0.6331 - val_accuracy: 0.6844\nEpoch 11/40\n - 1s - loss: 0.6096 - accuracy: 0.7042 - val_loss: 0.6338 - val_accuracy: 0.6837\nEpoch 12/40\n - 1s - loss: 0.6126 - accuracy: 0.7056 - val_loss: 0.6328 - val_accuracy: 0.6862\nEpoch 13/40\n - 1s - loss: 0.6088 - accuracy: 0.7044 - val_loss: 0.6320 - val_accuracy: 0.6856\nEpoch 14/40\n - 1s - loss: 0.6082 - accuracy: 0.7056 - val_loss: 0.6313 - val_accuracy: 0.6856\nEpoch 15/40\n - 1s - loss: 0.6076 - accuracy: 0.7044 - val_loss: 0.6309 - val_accuracy: 0.6869\nEpoch 16/40\n - 1s - loss: 0.6089 - accuracy: 0.7045 - val_loss: 0.6315 - val_accuracy: 0.6875\nEpoch 17/40\n - 1s - loss: 0.6086 - accuracy: 0.7052 - val_loss: 0.6317 - val_accuracy: 0.6869\nEpoch 18/40\n - 1s - loss: 0.6051 - accuracy: 0.7047 - val_loss: 0.6299 - val_accuracy: 0.6881\nEpoch 19/40\n - 1s - loss: 0.6028 - accuracy: 0.7063 - val_loss: 0.6299 - val_accuracy: 0.6888\nEpoch 20/40\n - 1s - loss: 0.6066 - accuracy: 0.7066 - val_loss: 0.6280 - val_accuracy: 0.6875\nEpoch 21/40\n - 1s - loss: 0.6044 - accuracy: 0.7055 - val_loss: 0.6276 - val_accuracy: 0.6888\nEpoch 22/40\n - 1s - loss: 0.6034 - accuracy: 0.7044 - val_loss: 0.6275 - val_accuracy: 0.6875\nEpoch 23/40\n - 1s - loss: 0.6022 - accuracy: 0.7073 - val_loss: 0.6299 - val_accuracy: 0.6875\nEpoch 24/40\n - 1s - loss: 0.6047 - accuracy: 0.7063 - val_loss: 0.6298 - val_accuracy: 0.6875\nEpoch 25/40\n - 1s - loss: 0.6034 - accuracy: 0.7069 - val_loss: 0.6285 - val_accuracy: 0.6875\nEpoch 26/40\n - 1s - loss: 0.6009 - accuracy: 0.7077 - val_loss: 0.6294 - val_accuracy: 0.6900\nEpoch 27/40\n - 1s - loss: 0.6013 - accuracy: 0.7066 - val_loss: 0.6303 - val_accuracy: 0.6894\nEpoch 28/40\n - 1s - loss: 0.6028 - accuracy: 0.7061 - val_loss: 0.6284 - val_accuracy: 0.6888\nEpoch 29/40\n - 1s - loss: 0.6000 - accuracy: 0.7078 - val_loss: 0.6278 - val_accuracy: 0.6881\nEpoch 30/40\n - 1s - loss: 0.6019 - accuracy: 0.7066 - val_loss: 0.6279 - val_accuracy: 0.6869\nEpoch 31/40\n - 1s - loss: 0.5998 - accuracy: 0.7086 - val_loss: 0.6289 - val_accuracy: 0.6894\nEpoch 32/40\n - 1s - loss: 0.6009 - accuracy: 0.7072 - val_loss: 0.6292 - val_accuracy: 0.6888\nEpoch 33/40\n - 1s - loss: 0.5978 - accuracy: 0.7083 - val_loss: 0.6277 - val_accuracy: 0.6888\nEpoch 34/40\n - 1s - loss: 0.5996 - accuracy: 0.7083 - val_loss: 0.6277 - val_accuracy: 0.6888\nEpoch 35/40\n - 1s - loss: 0.5966 - accuracy: 0.7069 - val_loss: 0.6271 - val_accuracy: 0.6894\nEpoch 36/40\n - 1s - loss: 0.5992 - accuracy: 0.7092 - val_loss: 0.6283 - val_accuracy: 0.6900\nEpoch 37/40\n - 1s - loss: 0.5995 - accuracy: 0.7070 - val_loss: 0.6282 - val_accuracy: 0.6888\nEpoch 38/40\n - 1s - loss: 0.6005 - accuracy: 0.7067 - val_loss: 0.6288 - val_accuracy: 0.6913\nEpoch 39/40\n - 1s - loss: 0.6006 - accuracy: 0.7077 - val_loss: 0.6299 - val_accuracy: 0.6888\nEpoch 40/40\n - 1s - loss: 0.5990 - accuracy: 0.7083 - val_loss: 0.6296 - val_accuracy: 0.6881\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1081c0550>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# !pip install tensorflow==1.14.0\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import SGD\n",
    "opt=SGD(lr=0.001)\n",
    "model = Sequential()\n",
    "# e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=1500, trainable=False)\n",
    "# model.add(e)\n",
    "model.add(Bidirectional(LSTM(128,input_shape=(1,X_train.shape[1]),dropout=0.2)))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "model.fit(trainX, y_train, epochs=40, batch_size=64, verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test score: 0.6164590644836426\nTest accuracy: 0.7014999985694885\n"
    }
   ],
   "source": [
    "score, acc= model.evaluate(testX, y_test,verbose=0)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "# print('F1 score:', f1_score)\n",
    "# print('Precision:', precision)\n",
    "# print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.40      0.01      0.01       595\n           1       0.70      1.00      0.82      1405\n\n    accuracy                           0.70      2000\n   macro avg       0.55      0.50      0.42      2000\nweighted avg       0.61      0.70      0.58      2000\n\n"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(testX)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "y_pred_b = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_pred_b, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}